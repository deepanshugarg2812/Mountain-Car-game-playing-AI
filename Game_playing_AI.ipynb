{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "for i in range(500):\n",
    "    env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Number of parameters\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Importing the libraries\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from collections import deque\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,state_size,action_size):\n",
    "        self.state_size = state_size ##Define the current state parameters eg X,Y cor or speed\n",
    "        self.action_size = action_size ##Define the number of actions that can be taken\n",
    "        self.gamma = 0.5 ##Penelty value\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.epislon = 1.0 ##Define the randomness in the system\n",
    "        self.epislon_decay = 0.995\n",
    "        self.epislon_min = 0.01\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._create_model()\n",
    "    def _create_model(self): ##Neural network for approxmating the future score\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24,input_dim = self.state_size,activation='relu'))\n",
    "        model.add(Dense(24,activation='relu'))\n",
    "        model.add(Dense(self.action_size,activation='linear'))\n",
    "        model.compile(loss='mse',optimizer=Adam(lr=0.001))\n",
    "        return model\n",
    "    def remember(self,state,action,reward,next_state,done): ##Remember the past\n",
    "        self.memory.append((state,action,reward,next_state,done))\n",
    "    def act(self,state):\n",
    "        if np.random.randn()<=self.epislon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state)[0])\n",
    "    def train(self,batch_size):\n",
    "        minibatch = random.sample(self.memory,batch_size)\n",
    "        for state,action,reward,next_state,done in minibatch:\n",
    "            if not done:\n",
    "                target = reward + self.gamma*np.amax(self.model.predict(next_state)[0])\n",
    "            else:\n",
    "                target = reward\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state,target_f,epochs=1,verbose=0)\n",
    "        if self.epislon > self.epislon_min:\n",
    "                self.epislon *= self.epislon_decay\n",
    "    \n",
    "    def load(self,name):\n",
    "        self.model.load_weights(name)\n",
    "    def save(self,name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=2,action_size=3)\n",
    "batch_size = 32\n",
    "n_episodes = 50\n",
    "output_dir = 'Mountain-cart/'\n",
    "state_size = 2\n",
    "action_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Episode :0/50, High Score:199,Exploration Rate:1.0\n",
      "Game Episode :1/50, High Score:199,Exploration Rate:0.99\n",
      "Game Episode :2/50, High Score:199,Exploration Rate:0.99\n",
      "Game Episode :3/50, High Score:199,Exploration Rate:0.99\n",
      "Game Episode :4/50, High Score:199,Exploration Rate:0.98\n",
      "Game Episode :5/50, High Score:199,Exploration Rate:0.98\n",
      "Game Episode :6/50, High Score:199,Exploration Rate:0.97\n",
      "Game Episode :7/50, High Score:199,Exploration Rate:0.97\n",
      "Game Episode :8/50, High Score:199,Exploration Rate:0.96\n",
      "Game Episode :9/50, High Score:199,Exploration Rate:0.96\n",
      "Game Episode :10/50, High Score:199,Exploration Rate:0.95\n",
      "Game Episode :11/50, High Score:199,Exploration Rate:0.95\n",
      "Game Episode :12/50, High Score:199,Exploration Rate:0.94\n",
      "Game Episode :13/50, High Score:199,Exploration Rate:0.94\n",
      "Game Episode :14/50, High Score:199,Exploration Rate:0.93\n",
      "Game Episode :15/50, High Score:199,Exploration Rate:0.93\n",
      "Game Episode :16/50, High Score:199,Exploration Rate:0.92\n",
      "Game Episode :17/50, High Score:199,Exploration Rate:0.92\n",
      "Game Episode :18/50, High Score:199,Exploration Rate:0.91\n",
      "Game Episode :19/50, High Score:199,Exploration Rate:0.91\n",
      "Game Episode :20/50, High Score:199,Exploration Rate:0.9\n",
      "Game Episode :21/50, High Score:199,Exploration Rate:0.9\n",
      "Game Episode :22/50, High Score:199,Exploration Rate:0.9\n",
      "Game Episode :23/50, High Score:199,Exploration Rate:0.89\n",
      "Game Episode :24/50, High Score:199,Exploration Rate:0.89\n",
      "Game Episode :25/50, High Score:199,Exploration Rate:0.88\n",
      "Game Episode :26/50, High Score:199,Exploration Rate:0.88\n",
      "Game Episode :27/50, High Score:199,Exploration Rate:0.87\n",
      "Game Episode :28/50, High Score:199,Exploration Rate:0.87\n",
      "Game Episode :29/50, High Score:199,Exploration Rate:0.86\n",
      "Game Episode :30/50, High Score:199,Exploration Rate:0.86\n",
      "Game Episode :31/50, High Score:199,Exploration Rate:0.86\n",
      "Game Episode :32/50, High Score:199,Exploration Rate:0.85\n",
      "Game Episode :33/50, High Score:199,Exploration Rate:0.85\n",
      "Game Episode :34/50, High Score:199,Exploration Rate:0.84\n",
      "Game Episode :35/50, High Score:199,Exploration Rate:0.84\n",
      "Game Episode :36/50, High Score:199,Exploration Rate:0.83\n",
      "Game Episode :37/50, High Score:199,Exploration Rate:0.83\n",
      "Game Episode :38/50, High Score:199,Exploration Rate:0.83\n",
      "Game Episode :39/50, High Score:199,Exploration Rate:0.82\n",
      "Game Episode :40/50, High Score:199,Exploration Rate:0.82\n",
      "Game Episode :41/50, High Score:199,Exploration Rate:0.81\n",
      "Game Episode :42/50, High Score:199,Exploration Rate:0.81\n",
      "Game Episode :43/50, High Score:199,Exploration Rate:0.81\n",
      "Game Episode :44/50, High Score:199,Exploration Rate:0.8\n",
      "Game Episode :45/50, High Score:199,Exploration Rate:0.8\n",
      "Game Episode :46/50, High Score:199,Exploration Rate:0.79\n",
      "Game Episode :47/50, High Score:199,Exploration Rate:0.79\n",
      "Game Episode :48/50, High Score:199,Exploration Rate:0.79\n",
      "Game Episode :49/50, High Score:199,Exploration Rate:0.78\n"
     ]
    }
   ],
   "source": [
    "for e in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state,(1,state_size))\n",
    "    for t in range(500):\n",
    "        action = agent.act(state)\n",
    "        next_state,reward,done,other_info = env.step(action) \n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state,[1,state_size])\n",
    "        agent.remember(state,action,reward,next_state,done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"Game Episode :{}/{}, High Score:{},Exploration Rate:{:.2}\".format(e,n_episodes,t,agent.epislon))\n",
    "            break\n",
    "            \n",
    "    if len(agent.memory)>batch_size:\n",
    "        agent.train(batch_size)\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
