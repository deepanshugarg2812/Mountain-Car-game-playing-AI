{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game play by AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,state_size,actions_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = actions_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.5\n",
    "        self.epislon = 1.0\n",
    "        self.epislon_decay = 0.995\n",
    "        self.epislon_min = 0.01\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._create_model()\n",
    "    def _create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24,input_dim = self.state_size,activation='relu'))\n",
    "        model.add(Dense(24,activation='relu'))\n",
    "        model.add(Dense(self.action_size,activation='linear'))\n",
    "        model.compile(loss='mse',optimizer=Adam(lr=0.001))\n",
    "        return model\n",
    "    def remember(self,state,action,reward,next_state,done):\n",
    "        ## Rember past experience\n",
    "        self.memory.append((state,action,reward,next_state,done))\n",
    "    def act(self,state):\n",
    "        if np.random.rand()<=self.epislon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state)[0])\n",
    "    def train(self,batch_size):\n",
    "        minibatch = random.sample(self.memory,batch_size)\n",
    "        for state,action,reward,next_state,done in minibatch:\n",
    "            if not done:\n",
    "                target = reward + self.gamma*np.amax(self.model.predict(next_state)[0])\n",
    "            else:\n",
    "                target = reward\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state,target_f,epochs=1,verbose=0)\n",
    "        if self.epislon > self.epislon_min:\n",
    "                self.epislon *= self.epislon_decay\n",
    "    \n",
    "    def load(self,name):\n",
    "        self.model.load_weights(name)\n",
    "    def save(self,name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=4,actions_size=2)\n",
    "batch_size = 32\n",
    "n_episodes = 10\n",
    "output_dir = \"cartpole_model/\"\n",
    "state_size = 4\n",
    "action_size =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Episode :0/10, High Score:12,Exploration Rate:0.96\n",
      "Game Episode :1/10, High Score:30,Exploration Rate:0.95\n",
      "Game Episode :2/10, High Score:15,Exploration Rate:0.95\n",
      "Game Episode :3/10, High Score:17,Exploration Rate:0.94\n",
      "Game Episode :4/10, High Score:20,Exploration Rate:0.94\n",
      "Game Episode :5/10, High Score:16,Exploration Rate:0.93\n",
      "Game Episode :6/10, High Score:17,Exploration Rate:0.93\n",
      "Game Episode :7/10, High Score:22,Exploration Rate:0.92\n",
      "Game Episode :8/10, High Score:12,Exploration Rate:0.92\n",
      "Game Episode :9/10, High Score:24,Exploration Rate:0.91\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "for e in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state,[1,state_size])\n",
    "    for t in range(500):\n",
    "        env.render()\n",
    "        action = agent.act(state) #action is 0 or 1\n",
    "        next_state,reward,done,other_info = env.step(action) \n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state,[1,state_size])\n",
    "        agent.remember(state,action,reward,next_state,done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"Game Episode :{}/{}, High Score:{},Exploration Rate:{:.2}\".format(e,n_episodes,t,agent.epislon))\n",
    "            break\n",
    "            \n",
    "    if len(agent.memory)>batch_size:\n",
    "        agent.train(batch_size)\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
